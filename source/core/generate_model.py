import os
import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
from huggingface_hub import snapshot_download

class GenerateModel:
    def __init__(self, 
                 peft_model_path="models/vit5-lora-med", 
                 base_model_repo="VietAI/vit5-large",
                 base_model_local="models/vit5-large"): 

        # T·ª± ƒë·ªông t·∫£i m√¥ h√¨nh g·ªëc n·∫øu ch∆∞a c√≥ local
        if not os.path.exists(base_model_local) or not os.path.exists(os.path.join(base_model_local, "config.json")):
            print("üîΩ ƒêang t·∫£i m√¥ h√¨nh base t·ª´ Hugging Face...")
            snapshot_download(
                repo_id=base_model_repo,
                local_dir=base_model_local,
                local_dir_use_symlinks=False
            )

        # BitsAndBytes config 
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=torch.bfloat16
        )

        # Load base model
        self.base_model = AutoModelForSeq2SeqLM.from_pretrained(
            base_model_local,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            use_safetensors=os.path.exists(os.path.join(base_model_local, "model.safetensors")),
            low_cpu_mem_usage=True
        )

        # Load adapter LoRA
        self.lora_model = PeftModel.from_pretrained(self.base_model, peft_model_path)

        # Load tokenizer t·ª´ adapter (ho·∫∑c d√πng base n·∫øu c·∫ßn)
        self.tokenizer = AutoTokenizer.from_pretrained(peft_model_path)

        # Setup device
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.lora_model.to(self.device)

        print("‚úÖ Generate model loaded successfully!")

    def answer(self, prompt, **generate_kwargs):
        if not prompt or not isinstance(prompt, str):
            return "L·ªói: Prompt kh√¥ng h·ª£p l·ªá."

        try:
            encoded_input = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=512,
                padding=True,
                truncation=True
            )
            
            encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}

            outputs = self.lora_model.generate(
                **encoded_input,
                max_length=400,
                min_length=20,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.2,
                no_repeat_ngram_size=3,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                **generate_kwargs
            )

            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            return response.strip()

        except torch.cuda.OutOfMemoryError:
            torch.cuda.empty_cache()
            return "L·ªói: H·∫øt b·ªô nh·ªõ GPU khi sinh vƒÉn b·∫£n."
        except Exception as e:
            return f"L·ªói trong qu√° tr√¨nh generate: {str(e)}"

    def generate_from_context(self, context: str, query: str, **kwargs):
        prompt = f"""
B·∫°n l√† b√°c sƒ© t∆∞ v·∫•n y t·∫ø. D∆∞·ªõi ƒë√¢y l√† th√¥ng tin y v·ªÅ b·ªánh li√™n quan (n·∫øu kh√¥ng li√™n quan th√¨ h√£y t·ª± tr·∫£ l·ªùi):
---------------------
{context}
---------------------
C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {query}
Ch·ªçn l·ªçc th√¥ng tin c·∫ßn thi·∫øt, tr·∫£ l·ªùi ng·∫Øn g·ªçn v√† ch√≠nh x√°c:
""".strip()

        return self.answer(prompt, **kwargs)
